Markov chains are the very basic building blocks of the theory used within this thesis. They are a natural extension of independent stochastic processes, that assume a weak dependence between the presence and the past.

In this thesis we will focus only on stochastic processes  with discrete time steps and finite state space, which fulfill(?) the Markov property. These are the ones that, we are able to simulate in computers.

\subsection{Basic terminology and assumptions}
	We assume that the reader has a basic probabilistic background, so that we can freely use terminology from probability theory, like random or independent variables, stochastic processes, measure or $\sigma$-algebra.
	
	Throughout the whole thesis we will be working on probabilistic space $\left\{ \Omega, \mathcal{F}, P \right\}$:
	\begin{definition}
		A probability space is a triplet: $\left\{ \Omega, \mathcal{F}, P \right\}$, where $\Omega$ is some abstract sample space
	\end{definition}
	is the space of all possible Markov chains, $\mathcal{F}$ a $\sigma$-algebra on $\Omega$ and $P$ is a probabilistic measure on this space.
	
	\begin{definition}
		The state space of a Markov chain is a finite set $S$
	\end{definition}

	\begin{definition}
		and because time is increasing in discrete steps, we will be working set of indices $I$ instead. In reality we have only finite amount of memory, so set $I$ will be also finite.
	\end{definition}
	
	Most of the time a Markov chain will be associated with a transition matrix $\PP$:
	\begin{definition}
		A transition matrix $\PP$ is a stochastic matrix, which means that the sum of rows is equal to $1$.
	\end{definition}
	
	
\subsection{Definition and basic properties}
	Formally a Markov chain is a stochastic process

\subsection{Stationarity}

\subsection{Ergodicity}