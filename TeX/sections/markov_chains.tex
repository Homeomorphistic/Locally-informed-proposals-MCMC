Markov chains are the very basic building blocks of the theory used within this thesis. They are a natural extension of independent stochastic processes, that assume a weak dependence between the presence and the past.

In this thesis we will focus only on stochastic processes  with discrete time steps and finite state space, which satisfies the Markov property. These are the ones that, we are able to simulate in computers.

\subsection{Basic terminology and assumptions}
	We assume that the reader has a basic probabilistic background, so that we can freely use terminology from probability theory, like random or independent variables, stochastic processes, measure or $\sigma$-algebra.
	
	A Markov chain needs to be defined with discrete state space and index set.
	\begin{definition}
		A state space of a Markov chain is a countable set $S$.
	\end{definition}
	A state space defines values over which a Markov chain is iterating. In our case it is finite so we can associate it with a subset of natural numbers like $\{1, 2, \ldots, N\}$ (for some $N$ depending on a number of states) instead of states.
	
	\begin{definition}
		An index set of a Markov chain is a countable set $T$.
	\end{definition}
	An index set represents time in which Markov chain moves. For a chain we assume discrete time steps and again in our case it will be a finite set, so we can associate it with a subset of natural numbers like $\{1, 2, \ldots, T\}$ for some $T$ depending on a length of time interval.
	
	To work with any probabilistic construct such as Markov chain, we need a probability space in which it resides and can be measured.
	\begin{definition}
		A probability space is a triplet $\left( \Omega, \mathcal{F}, P \right)$, where $\Omega$ is some abstract sample space, $\mathcal{F}$ is a $\sigma$-algebra (event space) and $P$ is a probability measure.
	\end{definition}
	In case of Markov chain sample space is a space of all possible Markov chains and an event space is a space of all events with Markov chains(?). Probability measure is not explicitly given, because it is often not possible to find a probability of a chain.
	
	Most of the time a Markov chain will be associated with a stochastic transition matrix $\PP$, that represents probabilities of transitions between states.
	\begin{definition}
		A stochastic matrix $\PP$ is a matrix, which rows sum to $1$.
	\end{definition}
	
	Now with the notation and the background we are able to define a Markov chain.
	
\subsection{Definition and basic properties}
	In this subsection we will formally define a Markov chain and list some of its basic properties.
	
	\begin{definition}
		A Markov chain (abbrv. MC) is a sequence of random variables $\left( X_n\right)_{n \in T}$ defined on a common probability space $\left( \Omega, \mathcal{F}, P \right)$, that take values in $S$, such that it satisfies Markov property:
		\begin{equation*}
			P(X_{n+m} = j | X_n = i, X_{l_{k-1}} = i_{l_{k-1}}, X_{l_{k-2}} = i_{l_{k-2}}, \ldots, X_{l_1} = i_1) = P(X_{n+m} = j | X_n = i).
		\end{equation*}
		For all indices $l_1< \ldots < l_{k-1} < n < n+m, \; 1 \leq k < n$ and all states $j, i, i_{n-1}, i_{n-2}, \ldots, i_0 \in S$.
	\end{definition}
	This definition indicates independence of the past of a MC. The probabilities depend on current state and the number of steps.

	\begin{definition}
		A Markov chain is homogeneous if additionally:
		\begin{equation*}
			P(X_{n+m} = j | X_m = i) = P(X_{n} = j | X_0 = i).
		\end{equation*}
		In this case we define:
		\begin{equation*}
			p_{i,j}(n) \overset{df}{=} P(X_{n} = j | X_0 = i).
		\end{equation*}
	\end{definition}
	Homogeneous Markov chains (abbrv. HMC) are more natural for us and easier to study. These eliminate the dependence on the number of steps that a Markov chain went through. From now on, whenever we use a term \textit{markov chain} we will mean a \textit{homogeneous markov chain}.
	
	The probabilities $p_{i,j}(n)$ give us a probability of transition between states $i$ and $j$ in $n$  steps. We can use them to form a special matrix that will be linked with a MC.
	\begin{definition}
		A transition matrix in $n$ steps $\PP(n)$ for a HMC is a stochastic matrix constructed using transition probabilities:
		\begin{equation*}
			\PP_{i,j}(n) = p_{i,j}(n), \; \PP_{i,j}(0) = \mathds{I}, \; \PP \overset{df}{=} \PP_{i,j}(1).
		\end{equation*}
	\end{definition}
	
	\begin{definition}
		A initial distribution of a Markov chain is a vector $\bfmu = (\mu_1, \ldots, \mu_N) \in \R^N$ such that $\sum_{i=1}^{N} \mu_i = 1$. This is a distribution of a random variable $X_0$, which is an initial state of a Markov chain.
	\end{definition}

	A transition matrix together with initial distribution define a HMC, so these are the only objects that need to be analyzed if one wants to study those chains.
	
	\begin{theorem}
		Let $\bfmu^{(n)}$ be a distribution of a HMC at $n$-th step, then for all $n$:
		\begin{equation*}
			\bfmu^{(n)} = P(X_n = j) =  \bfmu \PP^n.
		\end{equation*}
	\end{theorem}
	Proof of this theorem involves unfolding vectorized equation and using basic induction so it will be left. It also shows that given some knowledge of matrix $\PP$ one can easily work with HMC.
	
	\subsubsection{Irreducibility}
		Irreducibility guarantees that all states of a MC have the same properties, so that we do not need to analyze every state separately. Moreover an irreducible MC cannot be split into more chains.
		\begin{definition}[Irreducibility]
			A Markov chain with transition matrix $\PP$ is called irreducible if and only if for every pair of states $i$ and $j$ there exists a positive probability of transition between them \ie
			\begin{equation*}
				\exists \, n \, \PP_{i,j}(n) > 0.
			\end{equation*}
		\end{definition}
	
	\subsubsection{Periodicity}
		Periodicity tells us something about the structure of the transition matrix. It especially indicates when there is a possibility of a chain staying in one state.
		\begin{definition}[Periodicity]
			Let $d_i$ be a greatest common divisor of those $n$ such that $\PP_{i,i}(n)>0$ \ie
			\begin{equation*}
				d_i = gcd\{ n \geq : \PP_{i,i}(n)>0 \}
			\end{equation*}
			If $d_i > 1$ then state $i$ is periodic. If $d_i = 1$ then state $i$ is aperiodic.
		\end{definition}
	
		\begin{definition}
			A Markov chain with transition matrix $\PP$ is called periodic with a period $d$ when all states are periodic with a period $d$. In particular, when MC is irreducible and there is a state with a period $d$, then all the states are with period $d$ and chain is periodic.
		\end{definition}	
	

\subsection{Stationarity and ergodicity}
	In this subsection we will cover asymptotics for the long-term behavior of a MC.
	\begin{definition}[Stationarity]
		A probability distribution $\bfpi = (\pi_1, \ldots, \pi_N)^T$ is called stationary if it satisfies
		\begin{equation*}
			\pi_j = \sum_{i \in S} \pi_i p_{ij},
		\end{equation*}
		or equivalently in vector form:
		\begin{equation*}
			\bfpi = \bfpi \PP.
		\end{equation*}
	\end{definition}
	From this definition it is easy to see that if a MC gets to stationary distribution it will not leave it. 
	
	\begin{definition}[Ergodicity]
		When a MC is ergodic?
	\end{definition}
	
	\begin{theorem} \label{thm:existance_stationary}
		For any irreducible and aperiodic Markov chain, there exist at least one stationary distribution.
	\end{theorem}
	The proof of the theorem \ref{thm:existance_stationary} can be found in \cite{haggstrom2002finite}.
	
	\begin{theorem}
		Let $(X_n)$ be a irreducible and aperiodic HMC, then: ergodic???
		\begin{equation*}
			\lim_{n \rightarrow \infty} p_ij(n) = \pi_j.
		\end{equation*}
	\end{theorem}
	Proof?
	
	Those theorems tell us for some MC we are able to predict their behaviors after enough time passes. It also gives us a tool to sample from a given distribution $\bfpi$ using Markov chains.
	