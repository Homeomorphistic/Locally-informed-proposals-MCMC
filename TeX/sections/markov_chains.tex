Markov chains are the very basic building blocks of the theory used within this thesis. They are a natural extension of a sequence of independent random variables, that assume a weak dependence between the presence and the past.

In this thesis we will focus only on stochastic processes  with discrete time steps and finite state space, which satisfy the Markov property. These properties allow us to simulate processes in computers.

\subsection{Basic terminology and assumptions}
	We assume that the reader has a basic probabilistic background, so that we can freely use terminology from probability theory, like random or independent variables, stochastic processes, measure or $\sigma$-algebra.
	
	A Markov chain needs to be defined with discrete state space and index set.
	\begin{definition}
		A state space of a Markov chain is a countable set $S$.
	\end{definition}
	A state space defines values over which a Markov chain is iterating. In our case it is finite so we can associate it with natural numbers like $\{1, 2, \ldots, N\}$ (where $N$ is number of states) instead of states.
	
	\begin{definition}
		An index set of a Markov chain is a countable set $T$.
	\end{definition}
	An index set represents time in which Markov chain moves. For a chain we assume discrete time steps and again in our case it will be a finite set, so we can associate it with a subset of natural numbers like $\{1, 2, \ldots, t\}$ for some $t$ depending on a length of time interval.
	
	To work with any probabilistic construct such as Markov chain, we need a probability space in which it resides and can be measured.
	\begin{definition}
		A probability space is a triplet $\left( \Omega, \mathcal{F}, P \right)$, where $\Omega$ is some abstract sample space, $\mathcal{F}$ is a $\sigma$-algebra (event space) and $P$ is a probability measure.
	\end{definition}
	In our case the sample space is a common space for random variables $X_k, \; k=0,1,\ldots$ and is a space of all possible states. The event space is a space of all possible events given our states. Probability measure is not explicitly given, because it is often not easy to find a probability of a random variable.
	
	Most of the time a Markov chain will be associated with a stochastic transition matrix $\PP$, that represents probabilities of transitions between states.
	\begin{definition}
		A stochastic matrix $\PP$ is a matrix with non-negative entries, which rows sum to $1$.
	\end{definition}
	The number of entries of a transition matrix grows quadratically with number of states, so it quickly becomes a large object, not possible to fit into the memory of a computer. Now with the notation and the background we are able to define a Markov chain.
	
\subsection{Definition and basic properties}
	In this subsection we will formally define a Markov chain and list some of its basic properties.
	
	\begin{definition}
		A Markov chain (abbrv. MC) is a sequence of random variables $\left( X_k\right)_{k \in T}$ defined on a common probability space $\left( \Omega, \mathcal{F}, P \right)$, that take values in $S$, such that it satisfies Markov property:
		\begin{equation*}
			P(X_{k+m} = j | X_k = i, X_{l_{p-1}} = i_{l_{p-1}}, X_{l_{p-2}} = i_{l_{p-2}}, \ldots, X_{l_1} = i_1) = P(X_{k+m} = j | X_k = i),
		\end{equation*}
		for all indices $l_1< \ldots < l_{p-1} < k < k+m, \; 1 \leq p \leq k$, all states $j, i, i_{p-1}, i_{p-2}, \ldots, i_0 \in S$ and $m \geq 1$.
	\end{definition}
	This definition points out the independence of the past of a MC. The probabilities depend on current state and the number of steps.

	\begin{definition}
		A Markov chain is homogeneous if additionally:
		\begin{equation*}
			P(X_{k+m} = j | X_m = i) = P(X_{k} = j | X_0 = i).
		\end{equation*}
		In this case we define:
		\begin{equation*}
			p_{i,j}(k) \overset{df}{=} P(X_{k} = j | X_0 = i).
		\end{equation*}
	\end{definition}
	Homogeneous Markov chains are more natural for us and easier to study. These eliminate the dependence on the number of steps that a Markov chain went through. From now on, whenever we use a term \textit{Markov chain} we will mean a \textit{homogeneous Markov chain}.
	
	The probabilities $p_{i,j}(k)$ give us a probability of transition between states $i$ and $j$ in $k$  steps. We can use them to form a special matrix that will be linked with a MC.
	\begin{definition}
		A transition matrix in $k$ steps $\PP(k)$ for a Markov chain is a stochastic matrix constructed using transition probabilities:
		\begin{equation*}
			\PP_{i,j}(k) = p_{i,j}(k), \; \PP_{i,j}(0) = \mathds{I}, \; \PP \overset{df}{=} \PP_{i,j}(1).
		\end{equation*}
	\end{definition}
	
	\begin{definition}
		An initial distribution of a Markov chain is a vector $\bfmu = (\mu_1, \ldots, \mu_N) \in \R^N$ such that $\sum_{i=1}^{N} \mu_i = 1$. This is a distribution of a random variable $X_0$, which is an initial state of a Markov chain.
	\end{definition}

	A transition matrix together with initial distribution define a MC, so these are the only objects that need to be analyzed if one wants to study those chains.
	
	\begin{theorem}
		Let $\bfmu^{(k)} \in \R^N$ be a distribution of a Markov chain at $k$-th step and $\bfmu \in \R^N$ the initial distribution, then for all $k$:
		\begin{equation*}
			\bfmu^{(k)} = \bfmu \PP^k.
		\end{equation*}
	\end{theorem}
	Proof of this theorem involves unfolding vectorized equation and using basic induction so it will be left. It also shows that given some knowledge of matrix $\PP$ one can easily work with a MC.
	
	\subsubsection{Irreducibility}
		Irreducibility guarantees that all states of a MC are somehow connected and some of their properties transfer between them. We do not need to analyze every state separately. Moreover an irreducible MC cannot be split into more chains.
		\begin{definition}[Irreducibility]
			A Markov chain with transition matrix $\PP$ is called irreducible if and only if for every pair of states $i$ and $j$ there exists a positive probability of transition between them \ie
			\begin{equation*}
				\forall \; i,j \in S \; \exists \, k \, \PP_{i,j}(k) > 0.
			\end{equation*}
		\end{definition}
	
	\subsubsection{Periodicity}
		Periodicity tells us something about the structure of the transition matrix. It especially indicates when there is a possibility of a chain staying in one state.
		\begin{definition}[Periodicity]
			Let $d_i$ be a greatest common divisor of those $k$ such that $\PP_{i,i}(k)>0$ \ie
			\begin{equation*}
				d_i = gcd\{ k \geq : \PP_{i,i}(k)>0 \}
			\end{equation*}
			If $d_i > 1$ then state $i$ is periodic. If $d_i = 1$ then state $i$ is aperiodic.
		\end{definition}
	
		\begin{definition}
			A Markov chain with transition matrix $\PP$ is called periodic with a period $d$ when all states are periodic with a period $d$. In particular, when Markov chain is irreducible and there is a state with a period $d$, then all the states are with period $d$ and chain is periodic.
		\end{definition}	
	

\subsection{Stationarity and ergodicity}
	In this subsection we will cover asymptotics for the long-term behavior of a MC.
	\begin{definition}[Stationarity]
		A probability distribution $\bfpi = (\pi_1, \ldots, \pi_N)$ is called stationary if it satisfies
		\begin{equation*}
			\pi_j = \sum_{i \in S} \pi_i p_{ij},
		\end{equation*}
		or equivalently in vector form:
		\begin{equation*}
			\bfpi = \bfpi \PP.
		\end{equation*}
		This equation is often described as the balance equation.
	\end{definition}
	From this definition it is easy to see that if a MC starts with stationary distribution it will not leave it. 
	
	\begin{definition}[Ergodicity]
		A Markov chain is ergodic when it is irreducible and aperiodic.
	\end{definition}
	
	\begin{theorem} \label{thm:existance_stationary}
		For any ergodic Markov chain, there exist at least one stationary distribution.
	\end{theorem}
	
	
	\begin{theorem} \label{thm:convergance}
		Let $(X_n)$ be a ergodic Markov chain, then:
		\begin{equation*}
			\lim_{k \rightarrow \infty} p_{ij}(k) = \pi_j.
		\end{equation*}
	\end{theorem}
	The proofs of the Theorems \ref{thm:existance_stationary} and \ref{thm:convergance} can be found in \cite{haggstrom2002finite}. Those theorems tell us for some MC we are able to predict their behaviors after enough time passes. It also gives us a tool to approximate sampling from a given distribution $\bfpi$ using MC.
	
\subsection{Reversibility}
	A reversible MC has a property of having the same distribution in the past and in the future.
	\begin{definition}[Reversibility]
		A Markov chain $(X_k)$ is reversible if random vectors
		\begin{equation*}
			(X_{i_1}, X_{i_2}, \ldots, X_{i_l}) \; \mathrm{and} \; (X_{m-i_1}, X_{m-i_2}, \ldots, X_{m-i_l}),
		\end{equation*}
		have the same distribution for all $m$ and $i_j$, such that $i_j, m-i_j \in T$.
	\end{definition}
	
	This condition is just a mathematical representation of the previous statement and it is not practical, so we need another way of describing it.
	\begin{definition}[Detailed balance]
		A probability distribution $\bfpi = (\pi_1, \ldots, \pi_N)$ satisfies detailed balance for a Markov chain with transition matrix $\PP$ if
		\begin{equation*}
			\forall \; i,j \in S \; \pi_i p_{i,j} = \pi_j p_{j,i},
		\end{equation*}
		such distribution is called reversible.
	\end{definition}
	
	\begin{definition}
		A Markov chain is said to be reversible if there exists a reversible distribution for it.
	\end{definition}

	This definition is much more practical, because the condition can be calculated for each proposed distribution.
	
	\begin{theorem}
		If a probability distribution $\bfpi$ satisfies detailed balance for a Markov chain with transition matrix $\PP$, then $\bfpi$ is a stationary distribution for this chain.
	\end{theorem}
	\begin{proof}
		Summing over all $i \in S$ we get
		\begin{equation*}
			\sum_{i \in S} \pi_i p_{i,j} = \pi_j \sum_{i \in S} p_{j,i} = \pi_j,
		\end{equation*}
		which is a balance equation.
	\end{proof}

	The detailed balance will be often used in as it is somewhat easier to check than balance equation.
	
	
	