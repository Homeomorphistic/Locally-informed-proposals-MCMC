In the precious section we have seen that a deterministic approach of computing salesman tours is infeasible, so we can turn now to probabilistic methods of MCMC. Instead of checking all possible tours, now we want sample from this space, so the question is, how do we define a distribution there? It is usually done with a \textit{softmax}, a transformation of our target function, which is in this case a distance of a tour.

\begin{definition}
	For a given vector $\x = (x_1, x_2, \ldots, x_d)^T \in \R^d$ a softmax function $s: \R^d \rightarrow \left[0, 1\right]^d$ is defined as
	\begin{align*}
		s(\x)_i &= \frac{e^{x_i}}{\sum_{j=1}^{d} e^{x_j}}, \\
		s(\x) &= \left( s(\x)_1, s(\x)_2, \ldots, s(\x)_d \right).
	\end{align*}
\end{definition}
Such defined function is a probability distribution and it does not change the order of a given vector. This is important for us, because we can apply a softmax to the distances (weights) of the tours, so that we change them into distribution, but we will not change the relation between tours -- ones with longer distances will have greater probability. As we are interested in shortest distances, we can add a minus sign to give them the greatest probability.

Let us get back to the notation of weights and use a vector $\bfw = (w_1, w_2, \ldots, w_N)^T$ representing distances of all possible tours. As we noticed earlier, this space is enormous, so calculating softmax of this vector $s(\bfw)$ is again not feasible, because of enormous sum in the denominator $\sum_{j=1}^{N} e^{w_j}$. This is where Metropolis-Hastings algorithm comes in handy -- we do not need actually probabilities, we just need their quotients, and this is where the sums will erase each other. What we actually need is something proportional to the probability:
\begin{equation*}
	\pi_i \propto e^{-w_i}.
\end{equation*}
Using softmax is even more beneficial, because of the properties of exponential function. In each step of M-H algorithm we need to compute:
\begin{equation*}
	\frac{\pi_j \bfQ_{j,i}}{\pi_i \bfQ_{i,j}} = e^{-(w_j-w_i)} \cdot \frac{\bfQ_{j,i}}{\bfQ_{i,j}}.
\end{equation*}
If candidate matrix $\bfQ$ is symmetric this equation simplifies further. In practice we will be using logarithms of quotients in M-H algorithm, because multiplying numbers from $(0,1)$ get small quickly, so that they get out of the range of computer abilities. After applying logarithm we have:
\begin{equation*}
	\log \left( \frac{\pi_j \bfQ_{j,i}}{\pi_i \bfQ_{i,j}} \right) = -(w_j-w_i) + \log(\bfQ_{j,i}) - \log(\bfQ_{i,j}).
\end{equation*}
Again, if candidate matrix $\bfQ$ is symmetric we are left only with evaluating $w_j-w_i$ which is simple and it does not involve computing any other weights.

Now that we have a target distribution we can try sampling using a MC produced by M-H algorithm and after some point we should reach the tour with highest probability, \ie smallest distance covered. To do that we need to define candidate matrix $\bfQ$. The better the candidates, the faster MC can get to the minimum.

\subsection{Candidates}
	Let us start with considering what the candidate is. We know that the space of all possible tours is enormous, so we cannot take every tour into consideration. Instead we focus on, what we will call from now on \textit{neighbours}.
	\begin{definition}
		A neighbour $\sigma'$ of a permutation $\sigma$ is a permutation, that for some $k, l$ it satisfies $\sigma'(k) = \sigma(l)$, $\sigma'(l) = \sigma(k)$ and $\sigma'(i) = \sigma(i)$ for the rest of indices.
	\end{definition}
	These neighbours are the original tour with two swaped indices. This let's us consider a smaller space -- there are $\binom{n}{2} = \frac{n(n-1)}{2}$ neighbours if the number of vertices is $n$.

\subsection{Random neighbours}
	One approach is to sample neighbours uniformly, it has a benefit of being simple to understand and implement. It was already successfully presented in \cite{decryption_tsp_MCMC}. Sampling them uniformly is equivalent to choosing a random indices to swap, so it can be done efficiently. This way we do not need to create a candidate matrix, we just use a simple procedure. Most of the entries in this matrix would be $0$, because we choose only some subset of all possible permutations to be our neighbours. It is a symmetrical procedure (every neighbour has the same probability), so the step in M-H algorithm simplifies further.
	
	\subsubsection{Computational considerations}
		For this algorithm to be truly efficient, we need to find some other way to calculate distance (weight) of a tour, as it can also contain many edges. Without any optimizations for each tour $\sigma_i$ we need to calculate a sum:
		\begin{equation*}
			w_i = \sum_{i=1}^{n-1} w_{\sigma(i), \sigma(j)} + w_{\sigma(n), \sigma(1)}.
		\end{equation*}
		
		What was observed in \cite{decryption_tsp_MCMC} is that the weight does not change drastically when changing a tour to its neighbour. It is because most of the edges stay the same. When given a tour $\sigma$ and its neighbour $\sigma'$ they differ only on those edges where swap is happening, let's say $k,l$. So for this situation we have paths:
		\begin{align*}
			\sigma: \quad (e_{\sigma(1), \sigma(2)}, e_{\sigma(2), \sigma(3)} \ldots, e_{\sigma(k-1), \sigma(k)}, e_{\sigma(k), \sigma(k+1)}, \ldots, e_{\sigma(l-1), \sigma(l)}, e_{\sigma(l), \sigma(l+1)}, \ldots) \\
			\sigma': \quad(e_{\sigma(1), \sigma(2)}, e_{\sigma(2), \sigma(3)} \ldots, e_{\sigma(k-1), \sigma(l)}, e_{\sigma(l), \sigma(k+1)}, \ldots, e_{\sigma(l-1), \sigma(k)}, e_{\sigma(k), \sigma(l+1)}, \ldots)
		\end{align*}
		Assuming that we know the sum of weights for tour $\sigma$ to obtain the new one for neighbour $\sigma'$ we need to remove from it weights $w_{k-1, k}, w_{k, k+1}, w_{l-1, l}, w_{l, l+1}$ and add $w_{k-1, l}, w_{l, k+1}, w_{l-1, k}, w_{k, l+1}$. This is only $8$ operations per neighbour, a constant complexity cost.
		
	\subsubsection{Algorithm}
		To sum up all the information we present the algorithm for this method (\ref{alg:rnd_neigh}). As mentioned before, we will be working on logarithms, because they are better suited for computer computations.
		
		\begin{algorithm}
			\caption{Random neighbours algorithm}\label{alg:rnd_neigh}
			\begin{algorithmic}[1]
				\State{Choose a tour $\sigma \in S_n$.}
				\State{$X_0 \gets \sigma$}
				\State{Compute $w_\sigma$.}
				\For{$i = 1,2, \ldots$}
				\State{Sample $k, l \sim Unif\left\{1, 2, \ldots, n\right\}$ without replacement.}
				\State{Sample $U \sim Unif(0,1)$.}
				\State{$w_{\sigma'} \gets w_\sigma - (w_{k-1, k} + w_{k, k+1} + w_{l-1, l} + w_{l, l+1}) + (w_{k-1, l} + w_{l, k+1} + w_{l-1, k} + w_{k, l+1})$}
				
				\If{$\log(U) \leq \min \left(0, -(w_{\sigma'} - w_\sigma) \right)$}
				\State $X_{i+1}(k), X_{i+1}(l) \gets X_{i+1}(l), X_{i+1}(k)$
				\Else
				\State $X_{i+1} \gets X_i$
				\EndIf
				\EndFor
			\end{algorithmic}
		\end{algorithm}

\subsection{Locally-informed proposals}
	Locally-informed proposals are more complicated family of methods, they include more computational labor. The approach with uniform distribution of candidates is less complex, but forces us to make a lot of iterations. It is because choosing neighbours randomly conveys no information, so it is required for us to check a lot of neighbours until we find a better one. 
	
	This time, we want to compute a \textit{local} distribution of neighbours and sample them from it. It has to be done efficiently too, because number of neighbours grows quadratically with the number of vertices (so for \textit{dsj1000} its around 1 million neighbours).
	
	The observation here is similar to the previous one -- when we have weights for all neighbours of a starting tour, we can update them by a common factor, only some of them change in a different way.
	
	\subsubsection{Computational considerations}
	
	\subsubsection{Algorithm}