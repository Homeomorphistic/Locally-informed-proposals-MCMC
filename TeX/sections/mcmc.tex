In this section we will show how to use aforementioned properties of MC for solving problems. The idea is to use a MC to simulate complicated models and estimate relevant parameters.

The classical example is measuring the area under a curve or a figure, with counting how many randomly generated points are inside or outside the figure and estimating area as a ratio of points inside. Such a sequence of points is a sequence of random independent variables, which is also a MC, but without any dependence.

If we could generate a MC with a stationary distribution proportional to some function of our interest we could find its maximum, by counting frequencies of the states. That is the core idea behind Metropolis-Hastings algorithm.

\subsection{Metropolis-Hastings algorithm}
	We seek an algorithm constructing a MC, which has a stationary distribution of our given probability distribution $\bfpi$ ($\pi_i > 0$). One could of course find such a transition matrix $\PP$ that has stationary distribution $\bfpi$ but this does not avoid the problem of enormous state space -- the matrix $\PP$ would also be enormous. So it would be a feasible idea, when the state space is small and also deterministic algorithms are able to find solutions. 
	
	Regardless of this fact let us start with constructing such a matrix. Assume that we have another stochastic matrix $\bfQ$ which is irreducible and aperiodic and $\bfQ_{i,j} >0 \iff \bfQ_{j,i} > 0$. Let us consider a matrix defined as:
	\begin{equation} \label{eq:PQ}
		\PP_{i,j} = 
		\begin{cases}
		\bfQ_{i,j} \min \left(1, \frac{\pi_j \bfQ_{j,i}}{\pi_i \bfQ_{i,j}}\right) & \text{ if } i \neq j, \\ 
		1 - \sum_{j \in S \setminus \left\{ i \right\} } \PP_{i,j}& \text{ if } i = j.
		\end{cases}
	\end{equation} 
	
	\begin{theorem}
		A matrix defined in \ref{eq:PQ} is stochastic, irreducible, aperiodic and has a stationary distribution $\bfpi$.
	\end{theorem}
	\begin{proof}
		The matrix $\PP$ is stochastic from a definition -- one entry in a row is just a sum of every other and subtracted from 1. For $\bfQ_{i,j} > 0$ we have $\PP_{i,j} > 0$ and irreducibility and aperiodicity are inherited from $\bfQ$. Let us look at the detailed balance:
		\begin{equation*}
			\pi_j \PP_{j,i} = \pi_j \bfQ_{j,i} \min \left(1, \frac{\pi_i \bfQ_{i,j}}{\pi_j \bfQ_{j,i}}\right) = \min \left(\pi_i \bfQ_{i,j}, \pi_j \bfQ_{j,i} \right) =  \pi_i \bfQ_{i,j} \min \left(1, \frac{\pi_j \bfQ_{j,i}}{\pi_i \bfQ_{i,j}}\right) = \pi_i \PP_{i,j}.
		\end{equation*}
		We see that this matrix satisfies detailed balance, co the distribution $\bfpi$ is stationary.
	\end{proof}
	
	The matrix $\bfQ$ is called \textit{candidate matrix} or a \textit{proposal distribution} as it will be later proposing candidates for a MC. It is a kind of parameter for our later algorithms -- a well chosen matrix will give better or worse results. We proved more general case, but if matrix $\bfQ$ is symmetric some terms cancel and the proof becomes easier.
	
	The \textit{Metropolis-Hastings algorithm} (abbrv. M-H) utilizes this construction to generate irreducible, aperiodic MC with a stationary distribution $\bfpi$. When candidate matrix $\bfQ$ is symmetric we call this a \textit{Metropolis algorithm}.
	\input{algs/metropolis_hastings}
	
	
	In reality one does not need to create whole candidate matrix or a transition matrix. We just need to know how to sample at one step, so how to choose a candidate given a current state. If the procedure is symmetric it simplifies drastically, we need only to know the quotient of distribution $\bfpi$ at this step. The next constructions use this as an advantage to reduce number of computations.
	