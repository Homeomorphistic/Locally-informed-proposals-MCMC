The Markov Chain Monte Carlo methods (abbrv. MCMC) are a family of algorithms used for approximate sampling from a given probability distribution. At first they do not seem useful for solving practical deterministic problems, but with some tweaks they can become a powerful tool. It happens especially when space of possible solutions is enormous and computing becomes infeasible for machines. These offer a shortcut for obtaining ``close enough'' answers.
		
At their core, MCMC methods generate a Markov chain (abbrv. MC) with a defined distribution and sample using it. The convergence of the chain is assured by ergodic theorems. One of the most known of MCMC methods is is \textit{Metropolis-Hastings} algorithm, which constructs a MC using another set of distributions, maybe simpler ones.

In this thesis we work with \textit{locally-informed proposals}, they involve determining \textit{local} distribution -- which comes down to finding transition probabilities. They are a bit more complex and computationally heavy, but offer better results with less iterations. 

To test this method we will need a deterministic problem which quickly becomes infeasible for machines to compute -- one of them is a well-known traveling salesman problem. The testing is carried out using benchmark training set \textit{tsplib95} and implemen-tation is provided in \textit{Python3}.